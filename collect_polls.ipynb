{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmDuU7KFgA9YX4t7wfEWIh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yusufOcakoglu/Deciding_the_MVP/blob/main/collect_polls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Wahlrecht.de - Forsa şirketinin anketleri (HTML sayfası)\n",
        "url = \"https://www.wahlrecht.de/umfragen/forsa.htm\"\n",
        "\n",
        "try:\n",
        "    # Sayfadaki tüm tabloları bir liste olarak çeker.\n",
        "    # decimal=',' ve thousands='.' ayarları Alman formatını okumak için kritik!\n",
        "    tables = pd.read_html(url, decimal=',', thousands='.')\n",
        "\n",
        "    # Almanya anket verisi genellikle listelenen ilk (index 0) veya ikinci (index 1) büyük tablodur.\n",
        "    # Muhtemelen 1. index (tables[1]) sana ana veriyi verecektir.\n",
        "    df_forsa = tables[1]\n",
        "\n",
        "    print(\"✅ Forsa Anket Verisi (Almanya) başarıyla çekildi!\")\n",
        "    print(\"\\nÇekilen Tablonun İlk 5 Satırı:\")\n",
        "    print(df_forsa.head())\n",
        "\n",
        "    # CSV Olarak Kaydet (Bir sonraki adımda temizlemek için)\n",
        "    df_forsa.to_csv(\"forsa_polls_germany_raw.csv\", index=False)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Tablo çekilirken hata oluştu: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfDKvfsTO9YH",
        "outputId": "0c5238f1-1229-456f-c8e5-c9cbdd8aa016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Forsa Anket Verisi (Almanya) başarıyla çekildi!\n",
            "\n",
            "Çekilen Tablonun İlk 5 Satırı:\n",
            "  Unnamed: 0 Unnamed: 1 CDU/CSU   SPD GRÜNE  FDP LINKE   AfD FW  BSW Sonstige  \\\n",
            "0   25112025        NaN    25 %  14 %  12 %  3 %  11 %  26 %  –  3 %      6 %   \n",
            "1   18112025        NaN    25 %  14 %  12 %  3 %  11 %  26 %  –    –      9 %   \n",
            "2   11112025        NaN    24 %  14 %  12 %  3 %  11 %  26 %  –  3 %      7 %   \n",
            "3   04112025        NaN    24 %  14 %  12 %  3 %  12 %  26 %  –  3 %      6 %   \n",
            "4   28102025        NaN    25 %  13 %  12 %  3 %  12 %  26 %  –  3 %      6 %   \n",
            "\n",
            "  Unnamed: 11 Nichtwähler/ Unentschl. Befragte       Zeitraum  \n",
            "0         NaN                    26 %     2501  18.11.–24.11.  \n",
            "1         NaN                    25 %     2502  11.11.–17.11.  \n",
            "2         NaN                    24 %     2503  04.11.–10.11.  \n",
            "3         NaN                    23 %     2500  28.10.–03.11.  \n",
            "4         NaN                    23 %     2502  21.10.–27.10.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install eurostat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JG2UJV8TCrt",
        "outputId": "8437fddb-41c7-407b-902c-2d353ea57eab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting eurostat\n",
            "  Downloading eurostat-1.1.1-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from eurostat) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from eurostat) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->eurostat) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->eurostat) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->eurostat) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->eurostat) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->eurostat) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->eurostat) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->eurostat) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->eurostat) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->eurostat) (1.17.0)\n",
            "Downloading eurostat-1.1.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: eurostat\n",
            "Successfully installed eurostat-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def process_migration_data(input_file, output_file):\n",
        "    # Check if input file exists\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: The file '{input_file}' was not found.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        print(f\"Reading {input_file}...\")\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # distinct columns needed based on your request and the file snippet\n",
        "        # geo = Country Code\n",
        "        # TIME_PERIOD = Time Period\n",
        "        # OBS_VALUE = Observation Value\n",
        "        required_columns = ['geo', 'TIME_PERIOD', 'OBS_VALUE']\n",
        "\n",
        "        # Check if columns exist\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"Error: The following columns were not found in the CSV: {missing_cols}\")\n",
        "            return\n",
        "\n",
        "        # Extract only the necessary columns\n",
        "        extracted_df = df[required_columns].copy()\n",
        "\n",
        "        # --- Handling Missing Values ---\n",
        "\n",
        "        # 1. Ensure OBS_VALUE is numeric.\n",
        "        # Sometimes statistical data uses symbols like \":\" or \"b\" for missing/break in series.\n",
        "        # errors='coerce' turns these non-numeric values into NaN (Not a Number)\n",
        "        extracted_df['OBS_VALUE'] = pd.to_numeric(extracted_df['OBS_VALUE'], errors='coerce')\n",
        "\n",
        "        # 2. Check for missing values\n",
        "        initial_count = len(extracted_df)\n",
        "        missing_count = extracted_df['OBS_VALUE'].isna().sum()\n",
        "\n",
        "        if missing_count > 0:\n",
        "            print(f\"Found {missing_count} rows with missing or invalid Observation Values.\")\n",
        "\n",
        "            # Option A: Drop rows with missing values (Selected approach)\n",
        "            extracted_df = extracted_df.dropna(subset=['OBS_VALUE'])\n",
        "            print(\"Dropped rows with missing values.\")\n",
        "\n",
        "            # Option B: Fill with 0 (Alternative - commented out)\n",
        "            # extracted_df['OBS_VALUE'] = extracted_df['OBS_VALUE'].fillna(0)\n",
        "        else:\n",
        "            print(\"No missing values found.\")\n",
        "\n",
        "        # Aggregate OBS_VALUE for unique geo and TIME_PERIOD combinations\n",
        "        # This handles cases where multiple entries exist for the same period and country.\n",
        "        extracted_df = extracted_df.groupby(['geo', 'TIME_PERIOD'], as_index=False)['OBS_VALUE'].sum()\n",
        "        print(\"Aggregated duplicate entries for 'geo' and 'TIME_PERIOD'.\")\n",
        "\n",
        "        # Sort by Time and Geo for better readability\n",
        "        extracted_df = extracted_df.sort_values(by=['geo', 'TIME_PERIOD'])\n",
        "\n",
        "        # Save to new CSV\n",
        "        extracted_df.to_csv(output_file, index=False)\n",
        "\n",
        "        print(f\"\\nSuccess! Processed data saved to '{output_file}'\")\n",
        "        print(f\"Original rows before aggregation: {initial_count}\")\n",
        "        print(f\"Final rows after processing and aggregation: {len(extracted_df)}\")\n",
        "        print(\"\\nFirst 5 rows of the new file:\")\n",
        "        print(extracted_df.head())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Input filename based on your upload\n",
        "    input_csv = 'migr_asyappctzm__custom_19122438_linear_2_0.csv'\n",
        "\n",
        "    # Output filename\n",
        "    output_csv = 'processed_migration_data.csv'\n",
        "\n",
        "    process_migration_data(input_csv, output_csv)"
      ],
      "metadata": {
        "id": "MGtVC4_I4jcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ee7dca-aac5-4985-a545-8f170326657b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading migr_asyappctzm__custom_19122438_linear_2_0.csv...\n",
            "No missing values found.\n",
            "Aggregated duplicate entries for 'geo' and 'TIME_PERIOD'.\n",
            "\n",
            "Success! Processed data saved to 'processed_migration_data.csv'\n",
            "Original rows before aggregation: 5064\n",
            "Final rows after processing and aggregation: 518\n",
            "\n",
            "First 5 rows of the new file:\n",
            "  geo TIME_PERIOD  OBS_VALUE\n",
            "0  DE     2015-01      99565\n",
            "1  DE     2015-02     103520\n",
            "2  DE     2015-03     127720\n",
            "3  DE     2015-04     109105\n",
            "4  DE     2015-05     105395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def process_unemployment_data(input_file, output_file):\n",
        "    # Check if input file exists\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: The file '{input_file}' was not found.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        print(f\"Reading {input_file}...\")\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # distinct columns needed based on your request and the file snippet\n",
        "        # geo = Country Code\n",
        "        # TIME_PERIOD = Time Period\n",
        "        # OBS_VALUE = Observation Value\n",
        "        required_columns = ['geo', 'TIME_PERIOD', 'OBS_VALUE']\n",
        "\n",
        "        # Check if columns exist\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"Error: The following columns were not found in the CSV: {missing_cols}\")\n",
        "            return\n",
        "\n",
        "        # Extract only the necessary columns\n",
        "        extracted_df = df[required_columns].copy()\n",
        "\n",
        "        # --- Handling Missing Values ---\n",
        "\n",
        "        # 1. Ensure OBS_VALUE is numeric.\n",
        "        # Sometimes statistical data uses symbols like \":\" or \"b\" for missing/break in series.\n",
        "        # errors='coerce' turns these non-numeric values into NaN (Not a Number)\n",
        "        extracted_df['OBS_VALUE'] = pd.to_numeric(extracted_df['OBS_VALUE'], errors='coerce')\n",
        "\n",
        "        # 2. Check for missing values\n",
        "        initial_count = len(extracted_df)\n",
        "        missing_count = extracted_df['OBS_VALUE'].isna().sum()\n",
        "\n",
        "        if missing_count > 0:\n",
        "            print(f\"Found {missing_count} rows with missing or invalid Observation Values.\")\n",
        "\n",
        "            # Option A: Drop rows with missing values (Selected approach)\n",
        "            extracted_df = extracted_df.dropna(subset=['OBS_VALUE'])\n",
        "            print(\"Dropped rows with missing values.\")\n",
        "\n",
        "            # Option B: Fill with 0 (Alternative - commented out)\n",
        "            # extracted_df['OBS_VALUE'] = extracted_df['OBS_VALUE'].fillna(0)\n",
        "        else:\n",
        "            print(\"No missing values found.\")\n",
        "\n",
        "        # Sort by Time and Geo for better readability\n",
        "        extracted_df = extracted_df.sort_values(by=['geo', 'TIME_PERIOD'])\n",
        "\n",
        "        # Save to new CSV\n",
        "        extracted_df.to_csv(output_file, index=False)\n",
        "\n",
        "        print(f\"\\nSuccess! Processed data saved to '{output_file}'\")\n",
        "        print(f\"Original rows: {initial_count}\")\n",
        "        print(f\"Final rows: {len(extracted_df)}\")\n",
        "        print(\"\\nFirst 5 rows of the new file:\")\n",
        "        print(extracted_df.head())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Input filename based on your new upload\n",
        "    input_csv = 'une_rt_m_page_linear_2_0.csv'\n",
        "\n",
        "    # Output filename\n",
        "    output_csv = 'processed_unemployment_data.csv'\n",
        "\n",
        "    process_unemployment_data(input_csv, output_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3iwzmfX9oKU",
        "outputId": "0cf6d654-8d8c-4901-a885-a5f8932c2d81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading une_rt_m_page_linear_2_0.csv...\n",
            "No missing values found.\n",
            "\n",
            "Success! Processed data saved to 'processed_unemployment_data.csv'\n",
            "Original rows: 517\n",
            "Final rows: 517\n",
            "\n",
            "First 5 rows of the new file:\n",
            "  geo TIME_PERIOD  OBS_VALUE\n",
            "0  DE     2015-01        4.5\n",
            "1  DE     2015-02        4.5\n",
            "2  DE     2015-03        4.5\n",
            "3  DE     2015-04        4.4\n",
            "4  DE     2015-05        4.4\n"
          ]
        }
      ]
    }
  ]
}